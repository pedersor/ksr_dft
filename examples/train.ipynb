{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/path/to/.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rNhEG51NdptH"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "Change to GPU runtime: Runtime -> Change runtime type -> Hardware accelerator -> GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPITNqd4bfvQ"
      },
      "outputs": [],
      "source": [
        "# Check cuda version\n",
        "! nvcc --version"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k6SQkOTwbVuJ"
      },
      "source": [
        "The jaxlib version must correspond to the version of the existing CUDA installation you want to use, with e.g. `cuda111` for CUDA 11.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxdUG_Z5bt3g"
      },
      "outputs": [],
      "source": [
        "# For GPU runtime\n",
        "! pip install --upgrade pip\n",
        "! pip install --upgrade jaxlib==0.1.72+cuda111 jax==0.2.19 -f https://storage.googleapis.com/jax-releases/jax_releases.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install ksr-dft\n",
        "! git clone https://github.com/pedersor/ksr_dft.git\n",
        "! pip install ksr_dft"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9mFqtv7ePb0"
      },
      "source": [
        "## Import and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XJwljOiIeaJO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import scipy\n",
        "import jax\n",
        "from jax import random\n",
        "from jax import tree_util\n",
        "from jax.config import config\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from ksr_dft import datasets\n",
        "from ksr_dft import jit_scf\n",
        "from ksr_dft import losses\n",
        "from ksr_dft import neural_xc\n",
        "from ksr_dft import np_utils\n",
        "from ksr_dft import scf\n",
        "from ksr_dft import utils\n",
        "from ksr_dft import xc\n",
        "from ksr_dft import analysis\n",
        "from ksr_dft import ksr\n",
        "\n",
        "\n",
        "os.environ['XLA_FLAGS'] = '--xla_gpu_deterministic_reductions'\n",
        "# Set the default dtype as float64. Note: the dtype may switch from float64 to\n",
        "# float32 during e.g. evaluating/training a convolution neural net. \n",
        "config.update('jax_enable_x64', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get path to ksr_dft repo\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  # in Colab\n",
        "  KSR_DFT_PATH = Path('/content/ksr_dft/')\n",
        "except:\n",
        "  # running in local directory\n",
        "  KSR_DFT_PATH = Path('../')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load atomic systems dataset\n",
        "ions_dataset = datasets.Dataset(KSR_DFT_PATH / 'data/ions/dmrg', num_grids=513)\n",
        "grids = ions_dataset.grids"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train neural XC functional with Kohn-Sham regularizer (KSR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../images/sksr_global.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters = 560\n",
            "step 0, loss 2.087971552383939 in 73.64164400100708 sec\n",
            "Save checkpoint example_model/ckpt-00000\n",
            "step 1, loss 0.09922457311561972 in 2.027907133102417 sec\n",
            "step 2, loss 0.04648410970686181 in 2.2076478004455566 sec\n",
            "step 3, loss 0.04505530701520058 in 2.4542808532714844 sec\n",
            "step 4, loss 0.04133870938155233 in 2.117668390274048 sec\n",
            "step 5, loss 0.03480095597706702 in 1.8888216018676758 sec\n",
            "step 6, loss 0.02358349513211198 in 2.8329248428344727 sec\n",
            "step 7, loss 0.01308982869115781 in 2.187788963317871 sec\n",
            "step 8, loss 0.006527987203520393 in 2.594045877456665 sec\n",
            "step 9, loss 0.006270831470625427 in 1.9054477214813232 sec\n",
            "step 10, loss 0.006243509754087787 in 2.2791924476623535 sec\n",
            "Save checkpoint example_model/ckpt-00010\n",
            "step 11, loss 0.006042104814021789 in 2.465707302093506 sec\n",
            "step 12, loss 0.005671039070936974 in 3.165398359298706 sec\n",
            "step 13, loss 0.005262280266105735 in 2.6839404106140137 sec\n",
            "step 14, loss 0.005099102132806769 in 2.7878565788269043 sec\n",
            "step 15, loss 0.00499341054056067 in 1.9727513790130615 sec\n",
            "step 16, loss 0.00452534132617856 in 2.0246598720550537 sec\n",
            "step 17, loss 0.004457974388584622 in 1.8109228610992432 sec\n",
            "step 18, loss 0.004330272688107931 in 2.1228408813476562 sec\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 67\u001b[0m\n\u001b[1;32m     53\u001b[0m trainer\u001b[39m.\u001b[39msetup_optimization(\n\u001b[1;32m     54\u001b[0m     initial_checkpoint_index\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     55\u001b[0m     save_every_n\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[39m# energy_loss_weight=0.5,\u001b[39;00m\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     66\u001b[0m \u001b[39m# perform training optimization\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m trainer\u001b[39m.\u001b[39;49mdo_lbfgs_optimization(verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     69\u001b[0m \u001b[39m## Validate Ions\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[39m# set validation set\u001b[39;00m\n\u001b[1;32m     72\u001b[0m to_validate \u001b[39m=\u001b[39m [(\u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m)]\n",
            "File \u001b[0;32m~/code/jax_dft_dev/ksr_dft/ksr.py:394\u001b[0m, in \u001b[0;36mSpinKSR.do_lbfgs_optimization\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo_lbfgs_optimization\u001b[39m(\u001b[39mself\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 394\u001b[0m   _, loss, _ \u001b[39m=\u001b[39m scipy\u001b[39m.\u001b[39;49moptimize\u001b[39m.\u001b[39;49mfmin_l_bfgs_b(\n\u001b[1;32m    395\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnp_loss_and_grad_fn,\n\u001b[1;32m    396\u001b[0m       x0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflatten_init_params,\n\u001b[1;32m    397\u001b[0m       args\u001b[39m=\u001b[39;49m(verbose,),\n\u001b[1;32m    398\u001b[0m       \u001b[39m# Maximum number of function evaluations.\u001b[39;49;00m\n\u001b[1;32m    399\u001b[0m       maxfun\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimization_params[\u001b[39m'\u001b[39;49m\u001b[39mmax_train_steps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    400\u001b[0m       factr\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m    401\u001b[0m       m\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m    402\u001b[0m       pgtol\u001b[39m=\u001b[39;49m\u001b[39m1e-14\u001b[39;49m)\n\u001b[1;32m    403\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFinal loss = \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    404\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[0;32m~/code/jax_dft_dev/venv/lib/python3.8/site-packages/scipy/optimize/_lbfgsb_py.py:197\u001b[0m, in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39m# build options\u001b[39;00m\n\u001b[1;32m    186\u001b[0m opts \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdisp\u001b[39m\u001b[39m'\u001b[39m: disp,\n\u001b[1;32m    187\u001b[0m         \u001b[39m'\u001b[39m\u001b[39miprint\u001b[39m\u001b[39m'\u001b[39m: iprint,\n\u001b[1;32m    188\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mmaxcor\u001b[39m\u001b[39m'\u001b[39m: m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcallback\u001b[39m\u001b[39m'\u001b[39m: callback,\n\u001b[1;32m    195\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mmaxls\u001b[39m\u001b[39m'\u001b[39m: maxls}\n\u001b[0;32m--> 197\u001b[0m res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args\u001b[39m=\u001b[39;49margs, jac\u001b[39m=\u001b[39;49mjac, bounds\u001b[39m=\u001b[39;49mbounds,\n\u001b[1;32m    198\u001b[0m                        \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopts)\n\u001b[1;32m    199\u001b[0m d \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mgrad\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mjac\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    200\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    201\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mfuncalls\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mnfev\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    202\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mnit\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mnit\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m    203\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mwarnflag\u001b[39m\u001b[39m'\u001b[39m: res[\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[1;32m    204\u001b[0m f \u001b[39m=\u001b[39m res[\u001b[39m'\u001b[39m\u001b[39mfun\u001b[39m\u001b[39m'\u001b[39m]\n",
            "File \u001b[0;32m~/code/jax_dft_dev/venv/lib/python3.8/site-packages/scipy/optimize/_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    353\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    356\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[1;32m    360\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "File \u001b[0;32m~/code/jax_dft_dev/venv/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[1;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[0;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[1;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
            "File \u001b[0;32m~/code/jax_dft_dev/venv/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/code/jax_dft_dev/venv/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
            "File \u001b[0;32m~/code/jax_dft_dev/venv/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
            "File \u001b[0;32m~/code/jax_dft_dev/venv/lib/python3.8/site-packages/scipy/optimize/_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[1;32m     75\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
            "File \u001b[0;32m~/code/jax_dft_dev/venv/lib/python3.8/site-packages/scipy/optimize/_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[1;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
            "File \u001b[0;32m~/code/jax_dft_dev/ksr_dft/ksr.py:363\u001b[0m, in \u001b[0;36mSpinKSR.np_loss_and_grad_fn\u001b[0;34m(self, flatten_params, verbose)\u001b[0m\n\u001b[1;32m    361\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    362\u001b[0m \u001b[39m# Automatic differentiation.\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m train_set_loss, train_set_gradient \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss_value_and_grad_fn(\n\u001b[1;32m    364\u001b[0m     flatten_params)\n\u001b[1;32m    366\u001b[0m \u001b[39m# for benchmarking\u001b[39;00m\n\u001b[1;32m    367\u001b[0m train_set_loss\u001b[39m.\u001b[39mblock_until_ready()\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer = ksr.SpinKSR(grids)\n",
        "\n",
        "\n",
        "# set ML model for xc functional\n",
        "model_dir = 'example_model/'\n",
        "if not os.path.exists(model_dir):\n",
        "  os.makedirs(model_dir)\n",
        "\n",
        "# architecture\n",
        "network = neural_xc.build_sliding_net(\n",
        "    window_size=1,\n",
        "    num_filters_list=[16, 16, 16],\n",
        "    activation='swish',\n",
        ")\n",
        "init_fn, neural_xc_energy_density_fn = neural_xc.global_functional_sigma(\n",
        "    network, grids=grids)\n",
        "\n",
        "trainer.set_neural_xc_functional(\n",
        "    model_dir=model_dir,\n",
        "    neural_xc_energy_density_fn=neural_xc_energy_density_fn)\n",
        "\n",
        "\n",
        "\n",
        "# set initial params from init_fn\n",
        "key = jax.random.PRNGKey(0)\n",
        "trainer.set_init_model_params(init_fn, key)\n",
        "\n",
        "# set KS parameters\n",
        "trainer.set_ks_params(\n",
        "    num_iterations=6,\n",
        "    alpha=0.5,\n",
        "    alpha_decay=0.9,\n",
        "    enforce_reflection_symmetry=False,\n",
        "    num_mixing_iterations=1,\n",
        "    density_mse_converge_tolerance=-1.,\n",
        "    stop_gradient_step=-1,\n",
        ")\n",
        "\n",
        "## Train Ions\n",
        "\n",
        "# set training set\n",
        "to_train = [(1, 1), (2, 2)]\n",
        "training_set = ions_dataset.get_ions(to_train)\n",
        "trainer.set_training_set(training_set)\n",
        "\n",
        "# setup parameters associated with the optimization\n",
        "trainer.setup_optimization(\n",
        "    initial_checkpoint_index=0,\n",
        "    save_every_n=10,\n",
        "    max_train_steps=100,\n",
        "    # number of iterations skipped in energy loss evaluation,\n",
        "    # a value of -1 corresponds to using the final KS only.\n",
        "    num_skipped_energies=-1,\n",
        "    # can also modify energy vs density weight in loss function:\n",
        "    # energy_loss_weight=0.5,\n",
        ")\n",
        "\n",
        "# perform training optimization\n",
        "trainer.do_lbfgs_optimization(verbose=1)\n",
        "\n",
        "## Validate Ions\n",
        "\n",
        "# set validation set\n",
        "to_validate = [(3, 3)]\n",
        "validation_set = ions_dataset.get_ions(to_validate)\n",
        "trainer.set_validation_set(validation_set)\n",
        "# get optimal checkpoint from validation\n",
        "trainer.get_optimal_ckpt(model_dir)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "general_tester.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "139bc12e35f8c2f7bc43704bf075b93bfe0a254d67539bd63d68d0283bbc44e8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
